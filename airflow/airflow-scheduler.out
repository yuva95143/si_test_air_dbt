[2025-09-04T13:54:56.853+0000] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2025-09-04T13:54:56.882+0000] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-09-04T13:54:56.883+0000] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-09-04T13:54:56.887+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 193776
[2025-09-04T13:54:56.888+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T13:54:56.891+0000] {settings.py:63} INFO - Configured default timezone UTC
[2025-09-04T13:54:56.906+0000] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-09-04T13:59:56.926+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:04:56.953+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:09:56.983+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:14:57.008+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:19:57.034+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:21:37.342+0000] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: dbt_auto_csv_pipeline.validate_environment manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
	<TaskInstance: dbt_auto_csv_pipeline.check_dbt_installation manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:21:37.342+0000] {scheduler_job_runner.py:507} INFO - DAG dbt_auto_csv_pipeline has 0/16 running and queued tasks
[2025-09-04T14:21:37.342+0000] {scheduler_job_runner.py:507} INFO - DAG dbt_auto_csv_pipeline has 1/16 running and queued tasks
[2025-09-04T14:21:37.342+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_auto_csv_pipeline.validate_environment manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
	<TaskInstance: dbt_auto_csv_pipeline.check_dbt_installation manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:21:37.344+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_auto_csv_pipeline.validate_environment manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>, <TaskInstance: dbt_auto_csv_pipeline.check_dbt_installation manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-04T14:21:37.344+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='validate_environment', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-09-04T14:21:37.344+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'validate_environment', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:21:37.345+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='check_dbt_installation', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-09-04T14:21:37.345+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'check_dbt_installation', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:21:37.350+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'validate_environment', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:21:38.529+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/projectuser/airflow/dags/dbt_auto_csv_dag.py
[2025-09-04T14:21:38.663+0000] {task_command.py:467} INFO - Running <TaskInstance: dbt_auto_csv_pipeline.validate_environment manual__2025-09-04T14:21:35.088825+00:00 [queued]> on host INB-YVARADHARAJAN.Trellance.com
[2025-09-04T14:21:39.307+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'check_dbt_installation', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:21:40.450+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/projectuser/airflow/dags/dbt_auto_csv_dag.py
[2025-09-04T14:21:40.569+0000] {task_command.py:467} INFO - Running <TaskInstance: dbt_auto_csv_pipeline.check_dbt_installation manual__2025-09-04T14:21:35.088825+00:00 [queued]> on host INB-YVARADHARAJAN.Trellance.com
[2025-09-04T14:21:42.729+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='validate_environment', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=1, map_index=-1)
[2025-09-04T14:21:42.729+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='check_dbt_installation', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=1, map_index=-1)
[2025-09-04T14:21:42.733+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_auto_csv_pipeline, task_id=check_dbt_installation, run_id=manual__2025-09-04T14:21:35.088825+00:00, map_index=-1, run_start_date=2025-09-04 14:21:40.620569+00:00, run_end_date=2025-09-04 14:21:42.363304+00:00, run_duration=1.742735, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-09-04 14:21:37.343307+00:00, queued_by_job_id=1, pid=197878
[2025-09-04T14:21:42.734+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_auto_csv_pipeline, task_id=validate_environment, run_id=manual__2025-09-04T14:21:35.088825+00:00, map_index=-1, run_start_date=2025-09-04 14:21:38.714218+00:00, run_end_date=2025-09-04 14:21:38.889994+00:00, run_duration=0.175776, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-09-04 14:21:37.343307+00:00, queued_by_job_id=1, pid=197869
[2025-09-04T14:21:42.782+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:21:42.783+0000] {scheduler_job_runner.py:507} INFO - DAG dbt_auto_csv_pipeline has 0/16 running and queued tasks
[2025-09-04T14:21:42.783+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:21:42.784+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-04T14:21:42.784+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='generate_intelligent_models', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-09-04T14:21:42.785+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'generate_intelligent_models', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:21:42.791+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'generate_intelligent_models', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:21:43.954+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/projectuser/airflow/dags/dbt_auto_csv_dag.py
[2025-09-04T14:21:44.061+0000] {task_command.py:467} INFO - Running <TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [queued]> on host INB-YVARADHARAJAN.Trellance.com
[2025-09-04T14:21:44.639+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='generate_intelligent_models', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=1, map_index=-1)
[2025-09-04T14:21:44.642+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_auto_csv_pipeline, task_id=generate_intelligent_models, run_id=manual__2025-09-04T14:21:35.088825+00:00, map_index=-1, run_start_date=2025-09-04 14:21:44.114676+00:00, run_end_date=2025-09-04 14:21:44.268202+00:00, run_duration=0.153526, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-09-04 14:21:42.783658+00:00, queued_by_job_id=1, pid=197888
[2025-09-04T14:24:57.081+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:26:45.171+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:26:45.171+0000] {scheduler_job_runner.py:507} INFO - DAG dbt_auto_csv_pipeline has 0/16 running and queued tasks
[2025-09-04T14:26:45.172+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:26:45.172+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-04T14:26:45.173+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='generate_intelligent_models', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-09-04T14:26:45.173+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'generate_intelligent_models', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:26:45.177+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'generate_intelligent_models', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:26:46.438+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/projectuser/airflow/dags/dbt_auto_csv_dag.py
[2025-09-04T14:26:46.602+0000] {task_command.py:467} INFO - Running <TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [queued]> on host INB-YVARADHARAJAN.Trellance.com
[2025-09-04T14:26:47.336+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='generate_intelligent_models', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=2, map_index=-1)
[2025-09-04T14:26:47.340+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_auto_csv_pipeline, task_id=generate_intelligent_models, run_id=manual__2025-09-04T14:21:35.088825+00:00, map_index=-1, run_start_date=2025-09-04 14:26:46.653336+00:00, run_end_date=2025-09-04 14:26:46.843343+00:00, run_duration=0.190007, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-09-04 14:26:45.172365+00:00, queued_by_job_id=1, pid=198483
[2025-09-04T14:29:57.118+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:31:47.454+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:31:47.454+0000] {scheduler_job_runner.py:507} INFO - DAG dbt_auto_csv_pipeline has 0/16 running and queued tasks
[2025-09-04T14:31:47.454+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>
[2025-09-04T14:31:47.455+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-04T14:31:47.455+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='generate_intelligent_models', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-09-04T14:31:47.455+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'generate_intelligent_models', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:31:47.462+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_auto_csv_pipeline', 'generate_intelligent_models', 'manual__2025-09-04T14:21:35.088825+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_auto_csv_dag.py']
[2025-09-04T14:31:48.909+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/projectuser/airflow/dags/dbt_auto_csv_dag.py
[2025-09-04T14:31:49.101+0000] {task_command.py:467} INFO - Running <TaskInstance: dbt_auto_csv_pipeline.generate_intelligent_models manual__2025-09-04T14:21:35.088825+00:00 [queued]> on host INB-YVARADHARAJAN.Trellance.com
[2025-09-04T14:31:49.896+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_auto_csv_pipeline', task_id='generate_intelligent_models', run_id='manual__2025-09-04T14:21:35.088825+00:00', try_number=3, map_index=-1)
[2025-09-04T14:31:49.900+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_auto_csv_pipeline, task_id=generate_intelligent_models, run_id=manual__2025-09-04T14:21:35.088825+00:00, map_index=-1, run_start_date=2025-09-04 14:31:49.193949+00:00, run_end_date=2025-09-04 14:31:49.419799+00:00, run_duration=0.22585, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-09-04 14:31:47.454878+00:00, queued_by_job_id=1, pid=199146
[2025-09-04T14:31:54.384+0000] {dagrun.py:823} ERROR - Marking run <DagRun dbt_auto_csv_pipeline @ 2025-09-04 14:21:35.088825+00:00: manual__2025-09-04T14:21:35.088825+00:00, state:running, queued_at: 2025-09-04 14:21:35.108703+00:00. externally triggered: True> failed
[2025-09-04T14:31:54.385+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_auto_csv_pipeline, execution_date=2025-09-04 14:21:35.088825+00:00, run_id=manual__2025-09-04T14:21:35.088825+00:00, run_start_date=2025-09-04 14:21:36.258204+00:00, run_end_date=2025-09-04 14:31:54.385058+00:00, run_duration=618.126854, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-09-04 14:21:35.088825+00:00, data_interval_end=2025-09-04 14:21:35.088825+00:00, dag_hash=11e1f37081fe35262f6fedc011e900d7
[2025-09-04T14:34:57.143+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-04T14:37:45.333+0000] {scheduler_job_runner.py:272} INFO - Exiting gracefully upon receiving signal 15
[2025-09-04T14:37:45.451+0000] {process_utils.py:132} INFO - Sending 15 to group 193776. PIDs of all processes in the group: []
[2025-09-04T14:37:45.451+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 193776
[2025-09-04T14:37:45.451+0000] {process_utils.py:101} INFO - Sending the signal 15 to process 193776 as process group is missing.
[2025-09-04T14:37:45.455+0000] {process_utils.py:132} INFO - Sending 15 to group 193776. PIDs of all processes in the group: []
[2025-09-04T14:37:45.455+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 193776
[2025-09-04T14:37:45.456+0000] {process_utils.py:101} INFO - Sending the signal 15 to process 193776 as process group is missing.
[2025-09-04T14:37:45.456+0000] {scheduler_job_runner.py:1029} INFO - Exited execute loop
